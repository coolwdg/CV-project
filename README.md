# 情绪识别与心理对话项目

本项目结合了面部表情识别、图像与文本匹配以及与在线聊天机器人的交互，旨在实现基于面部情绪分析的心理支持对话系统。用户可以通过摄像头实时捕捉面部表情，系统将识别情绪并将其与输入的文本信息一同发送给在线聊天机器人，以获取专业的心理支持建议。

## 功能特点
1. **面部表情识别**：使用`FER`库识别面部表情，提供多种情绪类型的分析。
2. **图像与文本匹配**：借助`OpenCLIP`和`BertModel`将图像特征与预定义的情绪描述文本进行匹配，获取更精确的情绪表达。
3. **实时视频处理**：利用`OpenCV`和`YOLO`模型实时捕捉摄像头画面，并提取面部区域。
4. **在线聊天交互**：通过`Selenium`自动化操作与在线聊天机器人进行交互，发送用户输入的文本和面部图像，获取心理支持回复。
5. **用户界面**：使用`PyQt5`构建直观的用户界面，方便用户操作和查看对话记录。

## 环境要求
在运行此项目之前，请确保你已经安装了以下依赖库，你可以使用以下命令进行安装：
```bash
pip install moviepy==2.0.0 Pillow torch open_clip numpy transformers matplotlib fer opencv-python PyQt5 ultralytics selenium
```
此外，你还需要下载以下模型文件并放置在相应的目录中：
- `best.pt`：YOLO目标检测模型，用于面部检测。
- `IDEA-CCNL_Taiyi-CLIP-RoBERTa-326M-ViT-H-Chinese`：Bert文本编码模型和相关的分词器。

同时，你需要安装Chrome浏览器和对应的ChromeDriver，确保`Selenium`能够正常工作。

## 项目结构
- `clip.py`：负责图像的情绪识别和文本匹配，包含表情描述`prompt`队列的定义、模型初始化和主流程的实现。
- `all.py`：实现了一个基于`PyQt5`的图形用户界面，包含视频流处理、聊天机器人交互和用户输入处理等功能。

